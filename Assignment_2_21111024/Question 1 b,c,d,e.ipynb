{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1 (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y = ax + b$  \n",
    "where,  \n",
    "a - slope of the line  \n",
    "b - y intercept of the line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function  \n",
    "The loss is the error in our predicted value of a and b  \n",
    "Our goal is to minimize the error to obtain the most accurate value of a and b "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "I am using the mean square error function to calculate the loss  \n",
    "$ E = \\frac{1}{n}\\sum_{i=0}^{n}((y_i - (ax_i + b))^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially let a = 0 and b = 0  \n",
    "Let learning rate be 0.0001 for good accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The partial derivative of the loss function with respect to a:  \n",
    "$ d_a = \\frac{-2}{n} \\sum_{i=0}^n (x_i)(y_i - (ax_i + b))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The partial derivative of loss function with respect to b:  \n",
    "$ d_b = \\frac{-2}{n} \\sum_{i=0}^n (y_i - (ax_i + b))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "update values of a and b  \n",
    "$a = a - learn\\_rate* d_a$  \n",
    "$b = b - learn\\_rate * d_b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat this process until our loss function is a very small value or 0  \n",
    "the value of a and b after this process will be our optimal values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function will return the optimal value of a and b\n",
    "def gradient_descent(learn_rate, n_iter = 2500, tol = 1e-06):\n",
    "    a = 0\n",
    "    b = 0\n",
    "    c = 0\n",
    "    d = 1\n",
    "    for _ in range(n_iter):\n",
    "        delta_a = -learn_rate * batch_gradient_a(a,b,c,d)\n",
    "        delta_b = -learn_rate * batch_gradient_b(a,b,c,d)\n",
    "        delta_c = -learn_rate * batch_gradient_c(a,b,c,d)\n",
    "        delta_d = -learn_rate * batch_gradient_d(a,b,c,d)\n",
    "        if np.all(np.abs(delta_a)<=tol) and np.all(np.abs(delta_b)<=tol):\n",
    "            break\n",
    "        a += delta_a\n",
    "        b += delta_b\n",
    "        c += delta_c\n",
    "        d += delta_d\n",
    "    return [round(a*1000)/1000, round(b*1000)/1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function find the derivative of loss function with respect to a\n",
    "def batch_gradient_a(a, b):\n",
    "    Y_pred = a*X + b\n",
    "    return (-2/n) * (X.T.dot(Y - Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function find the derivative of loss function with respect to b\n",
    "def batch_gradient_b(a, b):\n",
    "    Y_pred = a*X + b\n",
    "    return -2 * np.mean(Y - Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1 (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "n=10000\n",
    "X = 2.5 * np.random.rand(n) + 1.5\n",
    "res = 1.5 * np.random.rand(n)\n",
    "Y = 2 + 0.3 * X + res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.331, 2.654]\n",
      "time taken is  1.4470317363739014\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(batch_gradient_descent(0.01))\n",
    "print(\"time taken is \",time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1 (d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function will return the optimal value of a and b\n",
    "def minibatch_gradient_descent(learn_rate, no_of_batches, batch_size, n_iter = 2500, tol = 1e-06):\n",
    "    a = 0\n",
    "    b = 1\n",
    "    for _ in range(n_iter):\n",
    "        for _ in range(no_of_batches):\n",
    "            idx = random.sample(range(X.shape[0]),batch_size)\n",
    "            delta_a = -learn_rate * minibatch_gradient_a(a,b,idx)\n",
    "            delta_b = -learn_rate * minibatch_gradient_b(a,b,idx)\n",
    "            if np.all(np.abs(delta_a)<=tol) and np.all(np.abs(delta_b)<=tol):\n",
    "                break\n",
    "            a += delta_a\n",
    "            b += delta_b\n",
    "    return [round(a*1000)/1000, round(b*1000)/1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function find the derivative of loss function with respect to a\n",
    "def minibatch_gradient_a(a, b, idx):\n",
    "    Y_pred = a*X[idx] + b\n",
    "    return (-2/len(idx)) * (X[idx].T.dot(Y[idx] - Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function find the derivative of loss function with respect to b\n",
    "def minibatch_gradient_b(a, b, idx):\n",
    "    Y_pred = a*X[idx] + b\n",
    "    return -2 * np.mean(Y[idx] - Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.305, 2.733]\n",
      "time taken to converge 39.109859228134155\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(minibatch_gradient_descent(0.01, 10, 1000))\n",
    "print(\"time taken to converge\", time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1 (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function will return the optimal value of a and b\n",
    "def stochastic_gradient_descent(learn_rate, n_iter = 2500, tol = 1e-06):\n",
    "    a = 0\n",
    "    b = 1\n",
    "    for _ in range(n_iter):\n",
    "        idx = random.randint(0, X.shape[0]-1)\n",
    "        delta_a = -learn_rate * stochastic_gradient_a(a,b,idx)\n",
    "        delta_b = -learn_rate * stochastic_gradient_b(a,b,idx)\n",
    "        if np.all(np.abs(delta_a)<=tol) and np.all(np.abs(delta_b)<=tol):\n",
    "            break\n",
    "        a += delta_a\n",
    "        b += delta_b\n",
    "    return [round(a*1000)/1000, round(b*1000)/1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function find the derivative of loss function with respect to a\n",
    "def stochastic_gradient_a(a, b, idx):\n",
    "    Y_pred = a*X[idx] + b\n",
    "    return (-2) * (X[idx]*(Y[idx] - Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function find the derivative of loss function with respect to b\n",
    "def stochastic_gradient_b(a, b, idx):\n",
    "    Y_pred = a*X[idx] + b\n",
    "    return -2 * np.mean(Y[idx] - Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.396, 2.707]\n",
      "time taken to converge 0.07997655868530273\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(stochastic_gradient_descent(0.01))\n",
    "print(\"time taken to converge\",time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when no of epochs are fixed for both stochastic GD and batch GD, Stochastic GD takes less time than batch gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'i' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-478728c17387>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmintime\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mbest_batchsize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mminibatch_gradient_descent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'i' is not defined"
     ]
    }
   ],
   "source": [
    "mintime = np.inf\n",
    "best_batchsize = i\n",
    "for i in range(10,100,10):\n",
    "    start = time.time()\n",
    "    minibatch_gradient_descent(0.01,i,int(X.shape[0]/i))\n",
    "    endtime = time.time()-start\n",
    "    if mintime<endtime:\n",
    "        mintime = endtime\n",
    "        best_batchsize = i\n",
    "\n",
    "print(\"otimal batch size is\", best_batchsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The minibatch size which took the least time to coverge is 95"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9fca093f684e5e23aa26ff6116e81ad53d562196c00ead1a3caea9b3397a0987"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
